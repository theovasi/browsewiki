import os
import itertools
import math
import random
import shutil
import time
import xml.etree.ElementTree as ET
from xml.sax.saxutils import escape
import joblib
import argparse


def wiki_format(text_file_path,
                sub_size=None,
                output_file_path=os.getcwd()):
    """ Formats the collection so that there is one document per file.

        Before this method is run the collection is in the format generated by extracting
        the documents from a WikiPedia XML dump using Wikiextractor
        (github.com/attardi/wikiextractor).

        Arguments:
            sub_size (int, optional): Specifies the number of documents that will consist the
                formatted collection. Defaults to None and in this case the all the documents
                are kept during the formatting.
            output_file_path (str): The path to the file where the formatted collection will
                be saved. By default creates a file named 'formatted' in the current working
                directory.

        """

    # If a formatted collection directory already exists, overwrite it.
    if os.path.exists(output_file_path):
        shutil.rmtree(output_file_path)
        os.makedirs(output_file_path)

    n_docs = 0
    title_dict = dict()
    link_dict = dict()
    summary_dict = dict()
    filepath_dict = dict()
    start_time = time.time()

    for document_folder in os.listdir(text_file_path):
        os.makedirs(output_file_path + '/formatted/'+ document_folder)
        for document_file in os.listdir(text_file_path + '/' +
                                        document_folder):
            # The document's XML like format does not have a root element so it
            # needs to be added in order for the ElementTree to be created.
            with open(text_file_path + '/' + document_folder + '/' +
                      document_file) as document_file_content:
                # Escape all lines except <doc> tag lines to avoid XML parsing
                # errors
                document_file_content_escaped = []
                for line in document_file_content.readlines():
                    if (not line.startswith('<doc id') and
                            not line.startswith('</doc>')):
                        document_file_content_escaped.append(escape(line))
                    else:
                        document_file_content_escaped.append(line)

                document_file_iterator = \
                        itertools.chain('<root>', document_file_content_escaped, '</root>')
                # Parse the document file using the iterable.
                documents = ET.fromstringlist(document_file_iterator)
                # Each document file contains multiple documents each wrapped in a
                # doc tag
                for i, doc in enumerate(documents.findall("doc")):
                    # Pick documents at random from the whole collection.
                    if sub_size != None:
                        pos = float(sub_size) / (5 * pow(10, 6))
                        if random.random() > pos:
                            continue
                    doc_text = doc.text.splitlines()
                    title = doc_text[1]
                    link = title.replace(' ', '_')
                    summary = doc_text[3][:140]
                    # Save each document in a separate file.
                    filepath = '/'.join([
                        output_file_path, 'formatted', document_folder,
                        document_file + '_' + str(i)
                    ])

                    title_dict[n_docs] = title
                    link_dict[n_docs] = link
                    summary_dict[n_docs] = summary
                    filepath_dict[n_docs] = filepath

                    with open(filepath, 'wb+') as output_document_file:
                        output_document_file.write(doc.text.encode('utf-8'))
                    # If a subcollection size has been specified, stop when it is
                    # reached
                    n_docs += 1
                    print('Picked ' + str(n_docs) + '/' + str(sub_size) +
                          ' articles. ' + '(' +
                          str(math.floor(n_docs / (time.time() - start_time)))
                          + ' docs/s)')
                    if sub_size != None and n_docs >= sub_size:
                        joblib.dump(title_dict, output_file_path + '/title_dict.txt')
                        joblib.dump(link_dict, output_file_path + '/link_dict.txt')
                        joblib.dump(summary_dict, output_file_path + '/summary_dict.txt')
                        joblib.dump(filepath_dict, output_file_path + '/filepath_dict.txt')
                        return 0
        joblib.dump(title_dict, output_file_path + '/title_dict.txt')
        joblib.dump(link_dict, output_file_path + '/link_dict.txt')
        joblib.dump(summary_dict, output_file_path + '/summary_dict.txt')
        joblib.dump(filepath_dict, output_file_path + '/filepath_dict.txt')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Process input and output filepaths.')
    parser.add_argument('text_file_path', type=str,
                        help='The path to the text file that WikiExtractor created.')
    parser.add_argument('-s', '--size', type=int,
                        help='The size of the subcollection.')
    parser.add_argument('-o', '--output', type=str,
                        help='The location where the output files will be saved.')
    args = parser.parse_args()
    wiki_format(args.text_file_path, args.size, args.output)
